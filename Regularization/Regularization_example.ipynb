{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import libraries<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.Ridge_from_scratch import *\n",
    "from Model.Lasso_from_scratch import *\n",
    "from Model.ElasticNet_from_scratch import *\n",
    "from Metrics.Classification_metrics import *\n",
    "from Metrics.Regression_metrics import *\n",
    "from Plots.Prediction_plots import *\n",
    "Plots_predictions = Prediction_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Explanation of Regularization<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Regularization is the method used for the overfitting problem of linear and logistic regression.}$<p>\n",
    "$\\text{Overfitting, is when our algorithm returns good predictions on training data, but much weaker ones on unseen-test data.}$<p>\n",
    "$\\text{Generally speaking, when talking about any Machine Learning algorithm, it is important that it maximizes matching scores,}$\n",
    "$\\text{while minimizing error variance (of predictions and true values) between different sets of analyzed data.}$<p>\n",
    "$\\text{Adding a \"regularization term\" makes it likely that the model will increase the error slightly (on the training data), while it will decrease the variances.}$<p>\n",
    "$\\text{Each regularization algorithm reduces the complexity of the model, thus reducing overfitting.}$<p>\n",
    "$\\text{Trade-off between error and variance is a classic problem for any algorithm.}$<p>\n",
    "$\\text{The following section will present regularization algorithms for which the relationships will be true:}$<p>\n",
    "$\\text{- As } \\alpha \\text{ increases, the error }\\textbf{increases,}$<p>\n",
    "$\\text{- As } \\alpha \\text{ increases, the variance }\\textbf{decreases}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Advantages and disadvantages of Regularization<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Advantages of Regularization:}$<p>\n",
    "$\\text{- Reduces over-fitting by reducing the computational complexity of the model,}$<p>\n",
    "$\\text{- If there is correlation between variables it works better than regressions,}$<p>\n",
    "$\\text{- For a large number of variables, shorter compilation time than ordinary regressions.}$<br>\n",
    "\n",
    "$\\text{Disadvantages of Regularization:}$<p>\n",
    "$\\text{- Due to the reduction in the complexity of the model can lead to the phenomenon of mismatch (the model is too simple),}$<p>\n",
    "$\\text{- Regularization can make the model heavier to interpret due to the restrictions placed on the coefficients.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Explanation of Ridge Regression<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{The first regularization algorithm that I would like to explain in Ridge Regression.}$<p><br>\n",
    "\n",
    "$\\text{Ridge Regression is an } \\textbf{L2 regularization algorithm.}$<p>\n",
    "$\\text{Our linear regression minimization problem, for Ridge (after taking into account the \"regularization term\"), will look like this:}$<p>\n",
    "$$RSS_{Ridge}=\\sum_{i=1}^{N}\\left(y_i-{\\hat{\\beta}}_{Ridge}\\times X_i\\right)^2+\\alpha\\times\\sum_{m=0}^{M}{{\\hat{\\beta}}_{Ridge.\\ m}\\ }^2$$\n",
    "$\\text{Where: } y_i \\text{ - dependent variable for observation } i,$<p>\n",
    "${\\hat{\\beta}}_{Ridge} \\text{ - vector of estimated coefficients,}$<p>\n",
    "$X_i \\text{ - vector of independent variables for observation } i,$<p>\n",
    "$\\alpha \\text{ - shrinkage parameter,}$<p>\n",
    "$M \\text{ - the number of independent variables used in the model,}$<p>\n",
    "${\\hat{\\beta}}_{Ridge,\\ m} \\text{ - estimated coefficient value for variable m.}$<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Thus, when using gradient optimization:}$<p>\n",
    "$$Loss=\\frac{1}{N}\\times\\left(\\sum_{i=1}^{N}\\left(y_i-{\\hat{\\beta}}_{Ridge}\\times X_i\\right)^2+\\alpha\\times\\sum_{m=0}^{M}{{\\hat{\\beta}}_{Ridge,\\ m}\\ }^2\\right)=\\frac{1}{N}\\times\\left(\\sum_{i=1}^{N}\\left({y_i}^2-2\\times y_i\\times{\\hat{\\beta}}_{Ridge}\\times X_i+{{\\hat{\\beta}}_{Ridge}}^2\\times{X_i}^2\\right)+\\alpha\\times\\sum_{m=0}^{M}{{\\hat{\\beta}}_{Ridge,\\ m}\\ }^2\\right)$$\n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial{\\hat{\\beta}}_{Ridge}}=\\frac{1}{N}\\times\\left(\\sum_{i=1}^{N}\\left(-2\\times y_i\\times X_i+2\\times{\\hat{\\beta}}_{Ridge}\\times{X_i}^2\\right)+2\\times\\alpha\\times\\sum_{m=0}^{M}{\\hat{\\beta}}_{Ridge,\\ m}\\right)=-\\frac{2}{N}\\times\\left(\\sum_{i=1}^{N}\\left(X_i\\times\\left(y_i-{\\hat{\\beta}}_{Ridge}\\times X_i\\right)\\right)-\\alpha\\times\\sum_{m=0}^{M}{\\hat{\\beta}}_{Ridge,\\ m}\\right)$$\n",
    "\n",
    "$${\\hat{\\beta}}_{New}={\\hat{\\beta}}_{Old}+learning\\ rate\\times\\frac{2}{N}\\times\\left(\\sum_{i=1}^{N}\\left(X_i\\times\\left(y_i-{\\hat{\\beta}}_{Old}\\times X_i\\right)\\right)-\\alpha\\times\\sum_{m=0}^{M}{\\hat{\\beta}}_{Old,\\ m}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{If we would like to write this in matrix form then:}$<p>\n",
    "$${\\hat{\\beta}}_{Ridge}=\\left(X^\\prime\\times X+\\alpha\\times I\\right)^{-1}\\times X^\\prime\\times\\gamma$$\n",
    "$\\text{Where: } \\alpha \\text{ - shrinkage parameter (shrinkage parameter),}$<p>\n",
    "$I \\text{ - identity matrix.}$<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{We can also apply the Ridge algorithm to a classification problem.}$<p>\n",
    "$\\text{To do that we have to convert our dependent variables to: } \\{1,-1\\} \\text{ and treat the task as a regression problem.}$<p>\n",
    "$\\text{Thus, the calculations are exactly the same as for regression!}$<p>\n",
    "$\\text{It means that we will perform the determination of the optimal coefficient using the formula:}$\n",
    "\n",
    "$${\\hat{\\beta}}_{Ridge}=\\left(X^\\prime\\times X+\\alpha\\times I\\right)^{-1}\\times X^\\prime\\times\\gamma$$\n",
    "\n",
    "$\\text{In the case of multiclassing, we will have to break each class into separate subclasses by which we will}$<p>\n",
    "$\\text{obtain a coefficient matrix with dimensions (when the intercept is included in model): } [number\\ of\\ features+1;\\ number\\ of\\ classes].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Explanation of Lasso<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Lasso (Least Absolute Shrinkage Selector Operator) is an } \\textbf{L1 regularization algorithm.}$<p>\n",
    "$\\text{Our linear regression minimization problem, for Lasso (after taking into account the \"regularization term\"), will look like this:}$<p>\n",
    "\n",
    "$$RSS_{Lasso}=\\sum_{i=1}^{N}\\left(y_i-{\\hat{\\beta}}_{Lasso}\\times X_i\\right)^2+\\alpha\\times\\sum_{m=0}^{M}\\left|{\\hat{\\beta}}_{Lasso,m}\\right|$$\n",
    "\n",
    "$\\text{Where: } y_i \\text{ - dependent variable for observation i,}$<p>\n",
    "${\\hat{\\beta}}_{Lasso} \\text{ - vector of estimated coefficients,}$<p>\n",
    "$X_i \\text{ - vector of independent variables for observation i,}$<p>\n",
    "$\\alpha \\text{ - shrinkage parameter,}$<p>\n",
    "$M \\text{ - the number of independent variables used in the model,}$<p>\n",
    "${\\hat{\\beta}}_{Lasso,m} \\text{ - estimated coefficient value for variable m.}$<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Thus, when using gradient optimization:}$\n",
    "\n",
    "$$Loss=\\frac{1}{N}\\times\\left(\\sum_{i=1}^{N}\\left(y_i-{\\hat{\\beta}}_{Lasso}\\times X_i\\right)^2+\\alpha\\times\\sum_{m=0}^{M}\\left|{\\hat{\\beta}}_{Lasso,m}\\right|\\right)=\\frac{1}{N}\\times\\left(\\sum_{i=1}^{N}\\left({y_i}^2-2\\times y_i\\times{\\hat{\\beta}}_{Lasso}\\times X_i+{{\\hat{\\beta}}_{Lasso}}^2\\times{X_i}^2\\right)+\\alpha\\times\\sum_{m=0}^{M}\\left|{\\hat{\\beta}}_{Lasso,m}\\right|\\right)$$\n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial{\\hat{\\beta}}_{Lasso}}=\\frac{1}{N}\\times\\left(\\sum_{i=1}^{N}\\left(-2\\times y_i\\times X_i+2\\times{\\hat{\\beta}}_{Lasso}\\times{X_i}^2\\right)+\\alpha\\times\\sum_{m=0}^{M}\\frac{{\\hat{\\beta}}_{Lasso,m}}{\\left|{\\hat{\\beta}}_{Lasso,m}\\right|}\\right)=-\\frac{2}{N}\\times\\left(\\sum_{i=1}^{N}\\left(X_i\\times\\left(y_i-{\\hat{\\beta}}_{Lasso}\\times X_i\\right)\\right)-\\frac{\\alpha}{2}\\times\\sum_{m=0}^{M}\\frac{{\\hat{\\beta}}_{Lasso,m}}{\\left|{\\hat{\\beta}}_{Lasso,m}\\right|}\\right)$$\n",
    "\n",
    "$${\\hat{\\beta}}_{New}={\\hat{\\beta}}_{Old}+learning\\ rate\\times\\frac{2}{N}\\times\\left(\\sum_{i=1}^{N}\\left(X_i\\times\\left(y_i-{\\hat{\\beta}}_{Old}\\times X_i\\right)\\right)-\\frac{\\alpha}{2}\\times\\sum_{m=0}^{M}\\frac{{\\hat{\\beta}}_{Old,m}}{\\left|{\\hat{\\beta}}_{Old,m}\\right|}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{We can also apply the Lasso algorithm to a classification problem.}$<p>\n",
    "$\\text{To do that we have to convert our dependent variables to: } \\{1,-1\\} \\text{ and treat the task as a regression problem.}$<br>\n",
    "\n",
    "$\\text{In the case of multiclassing, we will have to break each class into separate subclasses by which we will}$<p>\n",
    "$\\text{obtain a coefficient matrix with dimensions (when the intercept is included in model): } [number\\ of\\ features+1;\\ number\\ of\\ classes].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Explanation of ElasticNet<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{ElasticNet is a combination of } \\textbf{L1 and L2 regularization algorithms.}$<p>\n",
    "$\\text{Our linear regression minimization problem, for ElasticNet (after taking into account both \"regularization terms\"), will look like this:}$<p>\n",
    "\n",
    "$$RSS_{Elastic}=\\sum_{i=1}^{N}\\left(y_i-{\\hat{\\beta}}_{Elastic}\\times X_i\\right)^2+\\alpha_1\\times\\sum_{m=0}^{M}\\left|{\\hat{\\beta}}_{Elastic,\\ m}\\right|+\\alpha_2\\times\\sum_{m=0}^{M}{{\\hat{\\beta}}_{Elastic,\\ m}}^2$$\n",
    "\n",
    "$\\text{Where: } y_i \\text{ - dependent variable for observation i,}$<p>\n",
    "${\\hat{\\beta}}_{Elastic} \\text{ - vector of estimated coefficients,}$<p>\n",
    "$X_i \\text{ - vector of independent variables for observation i,}$<p>\n",
    "$\\alpha \\text{ - shrinkage parameter,}$<p>\n",
    "$M \\text{ - the number of independent variables used in the model,}$<p>\n",
    "${\\hat{\\beta}}_{Elastic,m} \\text{ - estimated coefficient value for variable m.}$<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Thus, when using gradient optimization:}$\n",
    "\n",
    "$$Loss=\\frac{1}{N}\\times\\left(\\sum_{i=1}^{N}\\left(y_i-{\\hat{\\beta}}_{Elastic}\\times X_i\\right)^2+\\alpha_1\\times\\sum_{m=0}^{M}\\left|{\\hat{\\beta}}_{Elastic,\\ m}\\right|+\\alpha_2\\times\\sum_{m=0}^{M}{{\\hat{\\beta}}_{Elastic,\\ m}}^2\\right)=\\frac{1}{N}\\times\\left(\\sum_{i=1}^{N}\\left({y_i}^2-2\\times y_i\\times{\\hat{\\beta}}_{Elastic}\\times X_i+{{\\hat{\\beta}}_{Elastic}}^2\\times{X_i}^2\\right)+\\alpha_1\\times\\sum_{m=0}^{M}\\left|{\\hat{\\beta}}_{Elastic,\\ m}\\right|+\\alpha_2\\times\\sum_{m=0}^{M}{{\\hat{\\beta}}_{Elastic,\\ m}}^2\\right)$$\n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial{\\hat{\\beta}}_{Elastic}}=\\frac{1}{N}\\times\\left(\\sum_{i=1}^{N}\\left(-2\\times y_i\\times X_i+2\\times{\\hat{\\beta}}_{Elastic}\\times{X_i}^2\\right)+\\alpha_1\\times\\sum_{m=0}^{M}\\frac{{\\hat{\\beta}}_{Elastic,\\ m}}{\\left|{\\hat{\\beta}}_{Elastic,\\ m}\\right|}+2\\times\\alpha_2\\times\\sum_{m=0}^{M}{\\hat{\\beta}}_{Elastic,\\ m}\\right)=-\\frac{2}{N}\\times\\left(\\sum_{i=1}^{N}\\left(X_i\\times\\left(y_i-{\\hat{\\beta}}_{Elastic}\\times X_i\\right)\\right)-\\frac{\\alpha_1}{2}\\times\\sum_{m=0}^{M}\\frac{{\\hat{\\beta}}_{Elastic,\\ m}}{\\left|{\\hat{\\beta}}_{Elastic,\\ m}\\right|}-\\alpha_2\\times\\sum_{m=0}^{M}{\\hat{\\beta}}_{Elastic,\\ m}\\right)$$\n",
    "\n",
    "$${\\hat{\\beta}}_{New}={\\hat{\\beta}}_{Old}+learning\\ rate\\times\\frac{2}{N}\\times\\left(\\sum_{i=1}^{N}\\left(X_i\\times\\left(y_i-{\\hat{\\beta}}_{Old}\\times X_i\\right)\\right)-\\frac{\\alpha_1}{2}\\times\\sum_{m=0}^{M}\\frac{{\\hat{\\beta}}_{Elastic,\\ m}}{\\left|{\\hat{\\beta}}_{Elastic,\\ m}\\right|}-\\alpha_2\\times\\sum_{m=0}^{M}{\\hat{\\beta}}_{Elastic,\\ m}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{We can also apply the ElasticNet algorithm to a classification problem.}$<p>\n",
    "$\\text{To do that we have to convert our dependent variables to: } \\{1,-1\\} \\text{ and treat the task as a regression problem.}$<br>\n",
    "\n",
    "$\\text{In the case of multiclassing, we will have to break each class into separate subclasses by which we will}$<p>\n",
    "$\\text{obtain a coefficient matrix with dimensions (when the intercept is included in model): } [number\\ of\\ features+1;\\ number\\ of\\ classes].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Comparision of Regularizers<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{- Ridge (L2) works better than Lasso (L1) when we have a lot of statistically significant variables in the dataset, because it will keep them in the model,}$<p>\n",
    "$\\text{- Lasso works better than Ridge when we have few statistically significant variables in the dataset,}$<p>\n",
    "$\\text{- Lasso can be used for variable selection unlike Ridge,}$<p>\n",
    "$\\text{- In the case where we have a huge number of variables (for example, 10000) even if there is a correlation between them,}$<p>\n",
    "$\\text{Lasso should be better in terms of memory when possibly needing to store and use the algorithm again.}$<p>\n",
    "$\\text{In addition, for such a large number of variables, even using Ridge, we may have the phenomenon of overfitting, as there will be too many variables and noise,}$<p>\n",
    "$\\text{- Lasso's problem, on the other hand, is the removal of variables in situations where there is correlation between predictors,}$<p>\n",
    "$\\text{causing the algorithm to lose important information,}$<p>\n",
    "$\\text{- In such situations (like the one described two points above) that ElasticNet is useful, which on the one hand will remove some noise,}$<p>\n",
    "$\\text{but we have some chance of keeping the variables correlated (ElasticNet works best for large data sets).}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Download data<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/bank-balanced.csv\")\n",
    "X = data.drop(\"deposit\", axis=1)\n",
    "y = data[\"deposit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in data: 11162\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>deposit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2343</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>45</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1270</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2476</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>184</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         job  marital  education default  balance housing loan  contact  \\\n",
       "0   59      admin.  married  secondary      no     2343     yes   no  unknown   \n",
       "1   56      admin.  married  secondary      no       45      no   no  unknown   \n",
       "2   41  technician  married  secondary      no     1270     yes   no  unknown   \n",
       "3   55    services  married  secondary      no     2476     yes   no  unknown   \n",
       "4   54      admin.  married   tertiary      no      184      no   no  unknown   \n",
       "\n",
       "   day month  duration  campaign  pdays  previous poutcome deposit  \n",
       "0    5   may      1042         1     -1         0  unknown     yes  \n",
       "1    5   may      1467         1     -1         0  unknown     yes  \n",
       "2    5   may      1389         1     -1         0  unknown     yes  \n",
       "3    5   may       579         1     -1         0  unknown     yes  \n",
       "4    5   may       673         2     -1         0  unknown     yes  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of observations in data: {}\".format(len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Check for null data<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age          0.0\n",
       "job          0.0\n",
       "marital      0.0\n",
       "education    0.0\n",
       "default      0.0\n",
       "balance      0.0\n",
       "housing      0.0\n",
       "loan         0.0\n",
       "contact      0.0\n",
       "day          0.0\n",
       "month        0.0\n",
       "duration     0.0\n",
       "campaign     0.0\n",
       "pdays        0.0\n",
       "previous     0.0\n",
       "poutcome     0.0\n",
       "deposit      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Check dtypes of dataset<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "job          object\n",
       "marital      object\n",
       "education    object\n",
       "default      object\n",
       "balance       int64\n",
       "housing      object\n",
       "loan         object\n",
       "contact      object\n",
       "day           int64\n",
       "month        object\n",
       "duration      int64\n",
       "campaign      int64\n",
       "pdays         int64\n",
       "previous      int64\n",
       "poutcome     object\n",
       "deposit      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Divide our data into train and test sets<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=17, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Because of the assumption of a normal distribution of our continuous variables, it is worth modifying their distribution before modeling.}$<p>\n",
    "$\\text{For this purpose, the Z-score will be used, which is described by the following formula:}$\n",
    "\n",
    "$${\\hat{X}}_{m, i}=\\frac{X_{m, i}-{\\bar{X}}_m}{\\sigma_m}$$\n",
    "\n",
    "$\\text{Where: } {\\hat{X}}_{m, i} \\text{ - standardized observation } i \\text{ of variable } m,$<p>\n",
    "$X_{m, i} \\text{ - observation } i \\text{ of variable } m,$<p>\n",
    "${\\bar{X}}_m \\text{ - mean of variable } m,$<p>\n",
    "$\\sigma_m \\text{ - standard deviation of variable } m.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{To properly approach the modeling process, the test set should remain unknown until the prediction is made.}$<p>\n",
    "$\\text{For this reason, we will only learn the mean and standard deviation for the training set and transform}$<p>\n",
    "$\\text{the two data sets based on just these values.}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data_train = X_train.select_dtypes(include=\"object\")\n",
    "continous_data_train = X_train.select_dtypes(exclude=\"object\")\n",
    "categorical_data_test = X_test.select_dtypes(include=\"object\")\n",
    "continous_data_test = X_test.select_dtypes(exclude=\"object\")\n",
    "categorical_data_train = pd.get_dummies(categorical_data_train, drop_first=True, dtype=int)\n",
    "categorical_data_test = pd.get_dummies(categorical_data_test, drop_first=True, dtype=int)\n",
    "mean_train = np.mean(continous_data_train, axis=0)\n",
    "std_train = np.std(continous_data_train, axis=0)\n",
    "continous_data_train = (continous_data_train-mean_train)/std_train\n",
    "continous_data_test = (continous_data_test-mean_train)/std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([categorical_data_train, continous_data_train], axis=1)\n",
    "X_test_final = pd.concat([categorical_data_test, continous_data_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluation and Visualization<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{To verify how well our algorithms are able to perform, a cross-validation will be used on the training set (in order to average the results obtained).}$<p>\n",
    "$\\text{Then we will check whether the algorithms will perform equally well (or even better) on the test data.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Simple Cross-Validation class}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Validation():\n",
    "    def __init__(self, metric, algorithm_instance, cross_validation_instance):\n",
    "        metrics = {\"accuracy\": [lambda y, y_pred: accuracy_score(y, y_pred), \"preds\"],\n",
    "                    \"roc_auc\": [lambda y, y_pred: roc_auc_score(y, y_pred), \"probs\"],\n",
    "                    \"mse\": [lambda y, y_pred: mean_squared_error(y, y_pred), \"preds\"],\n",
    "                    \"rmse\": [lambda y, y_pred: mean_squared_error(y, y_pred)**0.5, \"preds\"],\n",
    "                    \"mae\": [lambda y, y_pred: mean_absolute_error(y, y_pred), \"preds\"]}\n",
    "        if metric not in metrics:\n",
    "            raise ValueError('Unsupported metric: {}'.format(metric))\n",
    "        self.eval_metric = metrics[metric][0]\n",
    "        self.metric_type = metrics[metric][1]\n",
    "        self.algorithm = algorithm_instance\n",
    "        self.cv = cross_validation_instance\n",
    "    \n",
    "    def fit(self, X, y, verbose=False):\n",
    "        X = self.check_X(X=X)\n",
    "        y = self.check_y(y=y)\n",
    "        self.train_scores, self.valid_scores = [], []\n",
    "        for iter, (train_idx, valid_idx) in enumerate(self.cv.split(X, y)):\n",
    "            X_train, X_valid = X[train_idx, :], X[valid_idx, :]\n",
    "            y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "            self.algorithm.fit(X_train, y_train)\n",
    "            if(self.metric_type == \"preds\"):\n",
    "                y_train_pred = self.algorithm.predict(X_train)\n",
    "                y_valid_pred = self.algorithm.predict(X_valid)\n",
    "            else:\n",
    "                y_train_pred = self.algorithm.predict_proba(X_train)[:, 1]\n",
    "                y_valid_pred = self.algorithm.predict_proba(X_valid)[:, 1]\n",
    "            self.train_scores.append(self.eval_metric(y_train, y_train_pred))\n",
    "            self.valid_scores.append(self.eval_metric(y_valid, y_valid_pred))\n",
    "            if(verbose == True):\n",
    "                print(\"Iter {}: train scores: {}; valid scores: {}\".format(iter, np.round(self.eval_metric(y_train, y_train_pred), 5), np.round(self.eval_metric(y_valid, y_valid_pred), 5)))\n",
    "        return np.mean(self.train_scores), np.mean(self.valid_scores)\n",
    "    \n",
    "    def check_X(self, X):\n",
    "        if not isinstance(X, pd.DataFrame) and not isinstance(X, np.ndarray) and not torch.is_tensor(X):\n",
    "            raise TypeError('Wrong type of X. It should be dataframe, numpy array or torch tensor.')\n",
    "        X = np.array(X)\n",
    "        if(X.ndim == 1):\n",
    "            X = X[None, :]\n",
    "        return X\n",
    "    \n",
    "    def check_y(self, y):\n",
    "        if not isinstance(y, pd.DataFrame) and not isinstance(y, pd.Series) and not isinstance(y, np.ndarray) and not torch.is_tensor(y):\n",
    "            raise TypeError('Wrong type of y. It should be pandas DataFrame, pandas Series, numpy array or torch tensor.')\n",
    "        y = np.array(y)\n",
    "        if(y.ndim == 2):\n",
    "            y = y.squeeze()\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Ridge}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: train scores: 0.90176; valid scores: 0.90198\n",
      "Iter 1: train scores: 0.90251; valid scores: 0.89812\n",
      "Iter 2: train scores: 0.902; valid scores: 0.90082\n",
      "Iter 3: train scores: 0.90388; valid scores: 0.89291\n",
      "Iter 4: train scores: 0.90194; valid scores: 0.903\n",
      "Mean of train scores: 0.90242; Mean of valid scores: 0.89937\n"
     ]
    }
   ],
   "source": [
    "CV = Cross_Validation(metric=\"roc_auc\", algorithm_instance=Ridge_Classifier(alpha=1, fit_intercept=True), cross_validation_instance=KFold(n_splits=5, shuffle=True, random_state=17))\n",
    "mean_of_train_scores, mean_of_valid_scores = CV.fit(X=X_train_final, y=y_train, verbose=True)\n",
    "print(\"Mean of train scores: {}; Mean of valid scores: {}\".format(np.round(mean_of_train_scores, 5), np.round(mean_of_valid_scores, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Lasso}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: train scores: 0.86528; valid scores: 0.85678\n",
      "Iter 1: train scores: 0.83656; valid scores: 0.82935\n",
      "Iter 2: train scores: 0.84203; valid scores: 0.85691\n",
      "Iter 3: train scores: 0.85935; valid scores: 0.84001\n",
      "Iter 4: train scores: 0.84683; valid scores: 0.8431\n",
      "Mean of train scores: 0.85001; Mean of valid scores: 0.84523\n"
     ]
    }
   ],
   "source": [
    "CV = Cross_Validation(metric=\"roc_auc\", algorithm_instance=Lasso_Classifier(alpha=1, fit_intercept=True), cross_validation_instance=KFold(n_splits=5, shuffle=True, random_state=17))\n",
    "mean_of_train_scores, mean_of_valid_scores = CV.fit(X=X_train_final, y=y_train, verbose=True)\n",
    "print(\"Mean of train scores: {}; Mean of valid scores: {}\".format(np.round(mean_of_train_scores, 5), np.round(mean_of_valid_scores, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{ElasticNet}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: train scores: 0.86622; valid scores: 0.85773\n",
      "Iter 1: train scores: 0.83783; valid scores: 0.8306\n",
      "Iter 2: train scores: 0.84355; valid scores: 0.85801\n",
      "Iter 3: train scores: 0.86078; valid scores: 0.84162\n",
      "Iter 4: train scores: 0.84806; valid scores: 0.84455\n",
      "Mean of train scores: 0.85129; Mean of valid scores: 0.8465\n"
     ]
    }
   ],
   "source": [
    "CV = Cross_Validation(metric=\"roc_auc\", algorithm_instance=ElasticNet_Classifier(l1_term=1, l2_term=1, fit_intercept=True), cross_validation_instance=KFold(n_splits=5, shuffle=True, random_state=17))\n",
    "mean_of_train_scores, mean_of_valid_scores = CV.fit(X=X_train_final, y=y_train, verbose=True)\n",
    "print(\"Mean of train scores: {}; Mean of valid scores: {}\".format(np.round(mean_of_train_scores, 5), np.round(mean_of_valid_scores, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Logistic Regression}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: train scores: 0.90451; valid scores: 0.90404\n",
      "Iter 1: train scores: 0.90536; valid scores: 0.90014\n",
      "Iter 2: train scores: 0.90446; valid scores: 0.90353\n",
      "Iter 3: train scores: 0.90626; valid scores: 0.89701\n",
      "Iter 4: train scores: 0.90436; valid scores: 0.90488\n",
      "Mean of train scores: 0.90499; Mean of valid scores: 0.90192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "CV = Cross_Validation(metric=\"roc_auc\", algorithm_instance=LogisticRegression(penalty=None, fit_intercept=True), cross_validation_instance=KFold(n_splits=5, shuffle=True, random_state=17))\n",
    "mean_of_train_scores, mean_of_valid_scores = CV.fit(X=X_train_final, y=y_train, verbose=True)\n",
    "print(\"Mean of train scores: {}; Mean of valid scores: {}\".format(np.round(mean_of_train_scores, 5), np.round(mean_of_valid_scores, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Actually as we can see our Logistic Regression is doing better.}$<p>\n",
    "$\\text{Moreover train scores and valid scores are simmmilar for Logistic Regression therefore we can conclude that there might be no need to use Regularization.}$<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Just to be sure, compare predictions for train and test data sets.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: Train: 0.9022; Test: 0.9021\n",
      "Lasso: Train: 0.8645; Test: 0.8667\n",
      "ElasticNet: Train: 0.8365; Test: 0.832\n",
      "Logistic Regression: Train: 0.9047; Test: 0.9042\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge_Classifier(alpha=1, fit_intercept=True)\n",
    "lasso = Lasso_Classifier(alpha=1, fit_intercept=True)\n",
    "elastic = ElasticNet_Classifier(l1_term=1, l2_term=1, fit_intercept=True)\n",
    "logistic = LogisticRegression(penalty=None, fit_intercept=True)\n",
    "ridge.fit(X_train_final, y_train)\n",
    "y_ridge_prob_train = ridge.predict_proba(X_train_final)[:,1]\n",
    "y_ridge_prob_test = ridge.predict_proba(X_test_final)[:,1]\n",
    "print(\"Ridge: Train: {}; Test: {}\".format(np.round(roc_auc_score(y_train, y_ridge_prob_train), 4), np.round(roc_auc_score(y_test, y_ridge_prob_test), 4)))\n",
    "lasso.fit(X_train_final, y_train)\n",
    "y_lasso_prob_train = lasso.predict_proba(X_train_final)[:,1]\n",
    "y_lasso_prob_test = lasso.predict_proba(X_test_final)[:,1]\n",
    "print(\"Lasso: Train: {}; Test: {}\".format(np.round(roc_auc_score(y_train, y_lasso_prob_train), 4), np.round(roc_auc_score(y_test, y_lasso_prob_test), 4)))\n",
    "elastic.fit(X_train_final, y_train)\n",
    "y_elastic_prob_train = elastic.predict_proba(X_train_final)[:,1]\n",
    "y_elastic_prob_test = elastic.predict_proba(X_test_final)[:,1]\n",
    "print(\"ElasticNet: Train: {}; Test: {}\".format(np.round(roc_auc_score(y_train, y_elastic_prob_train), 4), np.round(roc_auc_score(y_test, y_elastic_prob_test), 4)))\n",
    "logistic.fit(X_train_final, np.array(y_train).squeeze())\n",
    "y_logistic_prob_train = logistic.predict_proba(X_train_final)[:,1]\n",
    "y_logistic_prob_test = logistic.predict_proba(X_test_final)[:,1]\n",
    "print(\"Logistic Regression: Train: {}; Test: {}\".format(np.round(roc_auc_score(np.array(y_train).squeeze(), y_logistic_prob_train), 4), np.round(roc_auc_score(np.array(y_test).squeeze(), y_logistic_prob_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Again, best results for Logistic Regression and again simmilar for train and test data set which means that algorithm is not overfitted.}$<p>\n",
    "$\\text{Obviuosly for some datasets there is no need to use regularization.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Regression<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{For regression we will use dataset with diabetes, and degrees of features to see how well our algorithm is able to predict with some noise added.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import load_diabetes\n",
    "X = load_diabetes()['data']\n",
    "y = load_diabetes()['target']\n",
    "poly = PolynomialFeatures(degree = 3, interaction_only=True)\n",
    "X_poly_transformed = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Divide our data into train and test sets<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_poly_transformed, y, shuffle=True, random_state=17, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluation and Visualization<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{To verify how well our algorithms are able to perform, a cross-validation will be used on the training set (in order to average the results obtained).}$<p>\n",
    "$\\text{Then we will check whether the algorithms will perform equally well (or even better) on the test data.}$\n",
    "\n",
    "$\\text{Using same cross validation class}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Ridge}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: train scores: 3332.89605; valid scores: 3862.14291\n",
      "Iter 1: train scores: 3477.34297; valid scores: 3655.18482\n",
      "Iter 2: train scores: 3435.1236; valid scores: 3447.49526\n",
      "Iter 3: train scores: 3546.01792; valid scores: 2654.42716\n",
      "Iter 4: train scores: 3353.94337; valid scores: 4158.55277\n",
      "Mean of train scores: 3429.06478; Mean of valid scores: 3555.56058\n"
     ]
    }
   ],
   "source": [
    "CV = Cross_Validation(metric=\"mse\", algorithm_instance=Ridge_Regressor(alpha=1, fit_intercept=True), cross_validation_instance=KFold(n_splits=5, shuffle=True, random_state=17))\n",
    "mean_of_train_scores, mean_of_valid_scores = CV.fit(X=X_train, y=y_train, verbose=True)\n",
    "print(\"Mean of train scores: {}; Mean of valid scores: {}\".format(np.round(mean_of_train_scores, 5), np.round(mean_of_valid_scores, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Lasso}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: train scores: 2756.83865; valid scores: 3444.59901\n",
      "Iter 1: train scores: 2876.97863; valid scores: 2697.05806\n",
      "Iter 2: train scores: 2780.87022; valid scores: 2995.2885\n",
      "Iter 3: train scores: 2857.21508; valid scores: 2692.37004\n",
      "Iter 4: train scores: 2751.77135; valid scores: 3264.88895\n",
      "Mean of train scores: 2804.73479; Mean of valid scores: 3018.84091\n"
     ]
    }
   ],
   "source": [
    "CV = Cross_Validation(metric=\"mse\", algorithm_instance=Lasso_Regressor(alpha=1, fit_intercept=True, learning_rate=0.3, max_iter=1500), cross_validation_instance=KFold(n_splits=5, shuffle=True, random_state=17))\n",
    "mean_of_train_scores, mean_of_valid_scores = CV.fit(X=X_train, y=y_train, verbose=True)\n",
    "print(\"Mean of train scores: {}; Mean of valid scores: {}\".format(np.round(mean_of_train_scores, 5), np.round(mean_of_valid_scores, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{ElasticNet}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: train scores: 2760.45038; valid scores: 3445.50135\n",
      "Iter 1: train scores: 2879.79203; valid scores: 2700.27571\n",
      "Iter 2: train scores: 2784.5875; valid scores: 2996.30815\n",
      "Iter 3: train scores: 2860.73193; valid scores: 2695.38855\n",
      "Iter 4: train scores: 2753.97096; valid scores: 3270.95494\n",
      "Mean of train scores: 2807.90656; Mean of valid scores: 3021.68574\n"
     ]
    }
   ],
   "source": [
    "CV = Cross_Validation(metric=\"mse\", algorithm_instance=ElasticNet_Regressor(l1_term=1, l2_term=1, fit_intercept=True, learning_rate=0.3, max_iter=1500), cross_validation_instance=KFold(n_splits=5, shuffle=True, random_state=17))\n",
    "mean_of_train_scores, mean_of_valid_scores = CV.fit(X=X_train, y=y_train, verbose=True)\n",
    "print(\"Mean of train scores: {}; Mean of valid scores: {}\".format(np.round(mean_of_train_scores, 5), np.round(mean_of_valid_scores, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Linear Regression}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: train scores: 3699.72695; valid scores: 27979.6338\n",
      "Iter 1: train scores: 1233.44231; valid scores: 20710.24547\n",
      "Iter 2: train scores: 2869.92908; valid scores: 21294.60563\n",
      "Iter 3: train scores: 1877.69611; valid scores: 12042.07143\n",
      "Iter 4: train scores: 1114.85866; valid scores: 55752.4\n",
      "Mean of train scores: 2159.13062; Mean of valid scores: 27555.79127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "CV = Cross_Validation(metric=\"mse\", algorithm_instance=LinearRegression(fit_intercept=True), cross_validation_instance=KFold(n_splits=5, shuffle=True, random_state=17))\n",
    "mean_of_train_scores, mean_of_valid_scores = CV.fit(X=X_train, y=y_train, verbose=True)\n",
    "print(\"Mean of train scores: {}; Mean of valid scores: {}\".format(np.round(mean_of_train_scores, 5), np.round(mean_of_valid_scores, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Now check for our test data.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: Train: 3322.2976222304; Test: 3396.5909\n",
      "Lasso: Train: 2827.2603; Test: 3182.1703\n",
      "ElasticNet: Train: 2830.4023; Test: 3185.5175\n",
      "Linear Regression: Train: 2103.6062322946; Test: 30916.7191\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge_Regressor(alpha=1, fit_intercept=True)\n",
    "lasso = Lasso_Regressor(alpha=1, fit_intercept=True, learning_rate=0.3, max_iter=1500)\n",
    "elastic = ElasticNet_Regressor(l1_term=1, l2_term=1, fit_intercept=True, learning_rate=0.3, max_iter=1500)\n",
    "linear = LinearRegression(fit_intercept=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_ridge_pred_train = ridge.predict(X_train)\n",
    "y_ridge_pred_test = ridge.predict(X_test)\n",
    "print(\"Ridge: Train: {}; Test: {}\".format(np.round(mean_squared_error(y_train, y_ridge_pred_train), 10), np.round(mean_squared_error(y_test, y_ridge_pred_test), 4)))\n",
    "lasso.fit(X_train, y_train)\n",
    "y_lasso_pred_train = lasso.predict(X_train)\n",
    "y_lasso_pred_test = lasso.predict(X_test)\n",
    "print(\"Lasso: Train: {}; Test: {}\".format(np.round(mean_squared_error(y_train, y_lasso_pred_train), 4), np.round(mean_squared_error(y_test, y_lasso_pred_test), 4)))\n",
    "elastic.fit(X_train, y_train)\n",
    "y_elastic_pred_train = elastic.predict(X_train)\n",
    "y_elastic_pred_test = elastic.predict(X_test)\n",
    "print(\"ElasticNet: Train: {}; Test: {}\".format(np.round(mean_squared_error(y_train, y_elastic_pred_train), 4), np.round(mean_squared_error(y_test, y_elastic_pred_test), 4)))\n",
    "linear.fit(X_train, np.array(y_train).squeeze())\n",
    "y_linear_pred_train = linear.predict(X_train)\n",
    "y_linear_pred_test = linear.predict(X_test)\n",
    "print(\"Linear Regression: Train: {}; Test: {}\".format(np.round(mean_squared_error(np.array(y_train).squeeze(), y_linear_pred_train), 10), np.round(mean_squared_error(np.array(y_test).squeeze(), y_linear_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{In this dataset we can clearly see that a simple linear regression model gives best results for train data.}$<p>\n",
    "$\\text{However, on the valid/test sets predictions are really poor - our model is overfitted.}$<p>\n",
    "$\\text{On the other hand there are regularization algorithms with much more simmilar predictions to real values (compared to Linear Regression).}$<p>\n",
    "$\\text{We will tune our regularization models a bit to see if we can improve the results.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Ridge}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001; Mean of train scores: 2364.9751; Mean of valid scores: 2966.03738\n",
      "Alpha: 0.01; Mean of train scores: 2628.89097; Mean of valid scores: 2972.03753\n",
      "Alpha: 0.1; Mean of train scores: 2786.73719; Mean of valid scores: 3011.22406\n",
      "Alpha: 1; Mean of train scores: 3429.06478; Mean of valid scores: 3555.56058\n",
      "Alpha: 10; Mean of train scores: 5113.65518; Mean of valid scores: 5145.78677\n",
      "Alpha: 100; Mean of train scores: 5822.44381; Mean of valid scores: 5829.08668\n",
      "Alpha: 1000; Mean of train scores: 5917.19329; Mean of valid scores: 5920.72745\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    CV = Cross_Validation(metric=\"mse\", algorithm_instance=Ridge_Regressor(alpha=alpha, fit_intercept=True), cross_validation_instance=KFold(n_splits=5, shuffle=True, random_state=17))\n",
    "    mean_of_train_scores, mean_of_valid_scores = CV.fit(X=X_train, y=y_train, verbose=False)\n",
    "    print(\"Alpha: {}; Mean of train scores: {}; Mean of valid scores: {}\".format(alpha, np.round(mean_of_train_scores, 5), np.round(mean_of_valid_scores, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Lasso}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.001; Mean of train scores: 2804.53525; Mean of valid scores: 3018.63305\n",
      "Alpha: 0.01; Mean of train scores: 2804.72292; Mean of valid scores: 3018.81419\n",
      "Alpha: 0.1; Mean of train scores: 2804.73445; Mean of valid scores: 3018.82892\n",
      "Alpha: 1; Mean of train scores: 2804.73479; Mean of valid scores: 3018.84091\n",
      "Alpha: 10; Mean of train scores: 2805.69304; Mean of valid scores: 3024.94072\n",
      "Alpha: 100; Mean of train scores: 4376.87077; Mean of valid scores: 4942.15764\n",
      "Alpha: 1000; Mean of train scores: 201661.54105; Mean of valid scores: 203082.81363\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    CV = Cross_Validation(metric=\"mse\", algorithm_instance=Lasso_Regressor(alpha=alpha, fit_intercept=True, learning_rate=0.3, max_iter=1500), cross_validation_instance=KFold(n_splits=5, shuffle=True, random_state=17))\n",
    "    mean_of_train_scores, mean_of_valid_scores = CV.fit(X=X_train, y=y_train, verbose=False)\n",
    "    print(\"Alpha: {}; Mean of train scores: {}; Mean of valid scores: {}\".format(alpha, np.round(mean_of_train_scores, 5), np.round(mean_of_valid_scores, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{ElasticNet}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Ridge and Lasso algorithms showed that the predictions are better for smaller values of penalty terms.}$<p>\n",
    "$\\text{Therefore in tuning of ElasticNet there will be used only values from range: } [0.001, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 term: 0.001; L2 term: 0.001; Mean of train scores: 2805.65053; Mean of valid scores: 3019.55215\n",
      "L1 term: 0.001; L2 term: 0.01; Mean of train scores: 2807.87447; Mean of valid scores: 3021.6121\n",
      "L1 term: 0.001; L2 term: 0.1; Mean of train scores: 2808.07484; Mean of valid scores: 3021.83561\n",
      "L1 term: 0.001; L2 term: 1; Mean of train scores: 2808.09033; Mean of valid scores: 3021.84961\n",
      "L1 term: 0.01; L2 term: 0.001; Mean of train scores: 2805.34587; Mean of valid scores: 3019.28274\n",
      "L1 term: 0.01; L2 term: 0.01; Mean of train scores: 2807.71611; Mean of valid scores: 3021.46419\n",
      "L1 term: 0.01; L2 term: 0.1; Mean of train scores: 2808.05825; Mean of valid scores: 3021.82087\n",
      "L1 term: 0.01; L2 term: 1; Mean of train scores: 2808.08867; Mean of valid scores: 3021.84815\n",
      "L1 term: 0.1; L2 term: 0.001; Mean of train scores: 2804.79221; Mean of valid scores: 3018.92376\n",
      "L1 term: 0.1; L2 term: 0.01; Mean of train scores: 2806.34309; Mean of valid scores: 3020.17983\n",
      "L1 term: 0.1; L2 term: 0.1; Mean of train scores: 2807.89319; Mean of valid scores: 3021.67361\n",
      "L1 term: 0.1; L2 term: 1; Mean of train scores: 2808.07205; Mean of valid scores: 3021.8335\n",
      "L1 term: 1; L2 term: 0.001; Mean of train scores: 2804.74054; Mean of valid scores: 3018.85184\n",
      "L1 term: 1; L2 term: 0.01; Mean of train scores: 2804.7948; Mean of valid scores: 3018.95125\n",
      "L1 term: 1; L2 term: 0.1; Mean of train scores: 2806.47338; Mean of valid scores: 3020.34957\n",
      "L1 term: 1; L2 term: 1; Mean of train scores: 2807.90656; Mean of valid scores: 3021.68574\n"
     ]
    }
   ],
   "source": [
    "for l1_term in [0.001, 0.01, 0.1, 1]:\n",
    "    for l2_term in [0.001, 0.01, 0.1, 1]:\n",
    "        CV = Cross_Validation(metric=\"mse\", algorithm_instance=ElasticNet_Regressor(l1_term=l1_term, l2_term=l2_term, fit_intercept=True, learning_rate=0.3, max_iter=1500), cross_validation_instance=KFold(n_splits=5, shuffle=True, random_state=17))\n",
    "        mean_of_train_scores, mean_of_valid_scores = CV.fit(X=X_train, y=y_train, verbose=False)\n",
    "        print(\"L1 term: {}; L2 term: {}; Mean of train scores: {}; Mean of valid scores: {}\".format(l1_term, l2_term, np.round(mean_of_train_scores, 5), np.round(mean_of_valid_scores, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{For best values of tuned hyperparameters, check wether we could improve our test data scores.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: Train: 2408.2648219278; Test: 3623.3098\n",
      "Lasso: Train: 2827.065; Test: 3181.933\n",
      "ElasticNet: Train: 2827.3356; Test: 3182.0772\n",
      "Linear Regression: Train: 2103.6062322946; Test: 30916.7191\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge_Regressor(alpha=0.001, fit_intercept=True)\n",
    "lasso = Lasso_Regressor(alpha=0.001, fit_intercept=True, learning_rate=0.3, max_iter=1500)\n",
    "elastic = ElasticNet_Regressor(l1_term=0.1, l2_term=0.001, fit_intercept=True, learning_rate=0.3, max_iter=1500)\n",
    "linear = LinearRegression(fit_intercept=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_ridge_pred_train = ridge.predict(X_train)\n",
    "y_ridge_pred_test = ridge.predict(X_test)\n",
    "print(\"Ridge: Train: {}; Test: {}\".format(np.round(mean_squared_error(y_train, y_ridge_pred_train), 10), np.round(mean_squared_error(y_test, y_ridge_pred_test), 4)))\n",
    "lasso.fit(X_train, y_train)\n",
    "y_lasso_pred_train = lasso.predict(X_train)\n",
    "y_lasso_pred_test = lasso.predict(X_test)\n",
    "print(\"Lasso: Train: {}; Test: {}\".format(np.round(mean_squared_error(y_train, y_lasso_pred_train), 4), np.round(mean_squared_error(y_test, y_lasso_pred_test), 4)))\n",
    "elastic.fit(X_train, y_train)\n",
    "y_elastic_pred_train = elastic.predict(X_train)\n",
    "y_elastic_pred_test = elastic.predict(X_test)\n",
    "print(\"ElasticNet: Train: {}; Test: {}\".format(np.round(mean_squared_error(y_train, y_elastic_pred_train), 4), np.round(mean_squared_error(y_test, y_elastic_pred_test), 4)))\n",
    "linear.fit(X_train, np.array(y_train).squeeze())\n",
    "y_linear_pred_train = linear.predict(X_train)\n",
    "y_linear_pred_test = linear.predict(X_test)\n",
    "print(\"Linear Regression: Train: {}; Test: {}\".format(np.round(mean_squared_error(np.array(y_train).squeeze(), y_linear_pred_train), 10), np.round(mean_squared_error(np.array(y_test).squeeze(), y_linear_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{We improved our results just a bit for Lasso and ElasticNet.}$<p>\n",
    "$\\text{We can now see how regularization algorithms work and also when we should use them!}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
